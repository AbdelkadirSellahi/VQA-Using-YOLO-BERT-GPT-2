{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "4qG4ODhUo-S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics transformers\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "YwWJ5uM6pGsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "cSesb4k7pGp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure CUDA (GPU support) is available if possible, else use CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "ScTuUEKapGoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained YOLOv8 model\n",
        "yolo_model = YOLO('/content/drive/MyDrive/00_PFE/Object_Detection/Training_Results/Yolov8-V4/Results/runs/train/experiment/weights/best.pt').to(device)"
      ],
      "metadata": {
        "id": "kZiYIz4IpGke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label mapping\n",
        "label_mapping = [\n",
        "    \"flooded\", \"non flooded\", \"flooded,non flooded\", \"Yes\", \"No\",\n",
        "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
        "    \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\",\n",
        "    \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\",\n",
        "    \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\",\n",
        "    \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\"\n",
        "]"
      ],
      "metadata": {
        "id": "n8C6WapypGiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the question type mapping\n",
        "question_type_mapping = {\n",
        "    \"Condition_Recognition\": 0,\n",
        "    \"Yes_No\": 1,\n",
        "    \"Simple_Counting\": 2,\n",
        "    \"Complex_Counting\": 3\n",
        "}"
      ],
      "metadata": {
        "id": "CEVlcu2npGfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from YOLOv8\n",
        "def extract_yolo_features(image_path, model, device):\n",
        "    results = model(image_path)\n",
        "\n",
        "    # Initialize lists to store extracted features\n",
        "    boxes_list = []\n",
        "    conf_list = []\n",
        "    cls_list = []\n",
        "\n",
        "    for result in results:\n",
        "        if result.boxes is not None:\n",
        "            boxes = result.boxes.xyxy.to(device)  # Bounding box coordinates\n",
        "            confs = result.boxes.conf.to(device)  # Confidence scores\n",
        "            classes = result.boxes.cls.to(device)  # Class values\n",
        "            boxes_list.append(boxes)\n",
        "            conf_list.append(confs)\n",
        "            cls_list.append(classes)\n",
        "\n",
        "    # Combine features into a single tensor\n",
        "    if boxes_list:\n",
        "        features = torch.cat([torch.cat(boxes_list), torch.cat(conf_list).unsqueeze(1), torch.cat(cls_list).unsqueeze(1)], dim=1)\n",
        "    else:\n",
        "        features = torch.empty((0, 6), device=device)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "K5a0dx89pGcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VQADataset class\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, tokenizer, transform=None):\n",
        "        with open(annotations_file, 'r') as f:\n",
        "            self.annotations = json.load(f)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.img_to_annotations = self._group_by_image()\n",
        "\n",
        "    def _group_by_image(self):\n",
        "        img_to_annotations = defaultdict(list)\n",
        "        for idx, annotation in self.annotations.items():\n",
        "            img_to_annotations[annotation['Image_ID']].append(annotation)\n",
        "        return img_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_to_annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = list(self.img_to_annotations.keys())[idx]\n",
        "        annotations = self.img_to_annotations[image_id]\n",
        "        img_path = os.path.join(self.img_dir, image_id)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        questions = []\n",
        "        answers = []\n",
        "        question_types = []\n",
        "        for annotation in annotations:\n",
        "            inputs = self.tokenizer.encode_plus(\n",
        "                annotation['Question'],\n",
        "                add_special_tokens=True,\n",
        "                return_tensors='pt',\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=64\n",
        "            )\n",
        "            question = inputs['input_ids'].squeeze(0).to(device)\n",
        "            attention_mask = inputs['attention_mask'].squeeze(0).to(device)\n",
        "            answer_text = str(annotation['Ground_Truth'])\n",
        "            answer_idx = label_mapping.index(answer_text)\n",
        "            question_type_idx = question_type_mapping[annotation['Question_Type']]\n",
        "            questions.append((question, attention_mask))\n",
        "            answers.append(torch.tensor(answer_idx, device=device))\n",
        "            question_types.append(torch.tensor(question_type_idx, device=device))\n",
        "        return {\n",
        "            'image_path': img_path,\n",
        "            'questions': questions,\n",
        "            'attention_masks': [am for _, am in questions],\n",
        "            'answers': torch.stack(answers),\n",
        "            'question_types': torch.stack(question_types)\n",
        "        }"
      ],
      "metadata": {
        "id": "9u1VV0_cpGaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    batch_image_paths = [item['image_path'] for item in batch]\n",
        "    batch_questions = [q for item in batch for q, _ in item['questions']]\n",
        "    batch_attention_masks = [am for item in batch for _, am in item['questions']]\n",
        "    batch_answers = torch.cat([item['answers'] for item in batch])\n",
        "    batch_question_types = torch.cat([item['question_types'] for item in batch])\n",
        "    num_questions_per_image = [len(item['questions']) for item in batch]\n",
        "    return {\n",
        "        'image_paths': batch_image_paths,\n",
        "        'questions': batch_questions,\n",
        "        'attention_masks': batch_attention_masks,\n",
        "        'answers': batch_answers,\n",
        "        'question_types': batch_question_types,\n",
        "        'num_questions_per_image': num_questions_per_image\n",
        "    }"
      ],
      "metadata": {
        "id": "AMr2rhmypGXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, bert_model, gpt2_model, yolo_input_dim, hidden_dim, vocab_size):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.gpt2_model = gpt2_model\n",
        "        self.fc_yolo = nn.Linear(yolo_input_dim, hidden_dim)\n",
        "        self.fc_proj = nn.Linear(hidden_dim + 768, gpt2_model.config.n_embd)  # Project to GPT-2 input dimension\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, image_features, questions, attention_masks, num_questions_per_image):\n",
        "        image_features = [self.fc_yolo(image_feature) for image_feature in image_features]\n",
        "        image_features = torch.stack(image_features)\n",
        "\n",
        "        text_features = [self.bert_model(question.unsqueeze(0).to(image_features.device), attention_mask=attention_mask.unsqueeze(0).to(image_features.device)).pooler_output for question, attention_mask in zip(questions, attention_masks)]\n",
        "        text_features = torch.cat(text_features, dim=0)\n",
        "\n",
        "        expanded_image_features = []\n",
        "        for image_feature, num_questions in zip(image_features, num_questions_per_image):\n",
        "            expanded_image_features.append(image_feature.repeat(num_questions, 1))\n",
        "        expanded_image_features = torch.cat(expanded_image_features, dim=0)\n",
        "\n",
        "        combined_features = torch.cat((expanded_image_features, text_features), dim=1)\n",
        "        projected_features = self.fc_proj(combined_features)\n",
        "\n",
        "        gpt2_output = self.gpt2_model(inputs_embeds=projected_features.unsqueeze(1), return_dict=True).logits\n",
        "\n",
        "        logits = gpt2_output[:, -1, :]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aHWo285vpGUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer, BERT model, GPT-2 model, and VQA model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "num_classes = len(label_mapping)\n",
        "hidden_dim = 256\n",
        "vqa_model = VQAModel(bert_model=bert_model, gpt2_model=gpt2_model, yolo_input_dim=6, hidden_dim=hidden_dim, vocab_size=num_classes).to(device)"
      ],
      "metadata": {
        "id": "ZuukOVvppGRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model state\n",
        "vqa_model.load_state_dict(torch.load('/content/drive/MyDrive/00_PFE/VQA/Code-V3/VQAModel_Best.pth'))"
      ],
      "metadata": {
        "id": "MSsVfB7rphZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset and dataloader\n",
        "test_annotations_file = '/content/drive/MyDrive/00_PFE/DataSet/Visual_Question_Answering /FloodNet Challenge @ EARTHVISION 2021 - Track 2/Questions/Training Question.json'\n",
        "test_img_dir = '/content/drive/MyDrive/00_PFE/DataSet/Visual_Question_Answering /FloodNet Challenge @ EARTHVISION 2021 - Track 2/Images/Train_Image'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "test_dataset = VQADataset(test_annotations_file, test_img_dir, bert_tokenizer, transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "An6kiLcPphWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation function\n",
        "def validate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    question_type_correct = defaultdict(int)\n",
        "    question_type_total = defaultdict(int)\n",
        "    question_type_loss = defaultdict(float)\n",
        "    total_correct = 0\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
        "            image_paths = batch['image_paths']\n",
        "            questions = batch['questions']\n",
        "            attention_masks = batch['attention_masks']\n",
        "            answers = batch['answers']\n",
        "            question_types = batch['question_types']\n",
        "            num_questions_per_image = batch['num_questions_per_image']\n",
        "\n",
        "            # Extract features using YOLOv8\n",
        "            image_features_list = []\n",
        "            for image_path in image_paths:\n",
        "                features = extract_yolo_features(image_path, yolo_model, device)\n",
        "                if features.nelement() == 0:\n",
        "                    features = torch.zeros((1, 6), device=device)  # Initialize with zeros if no features found\n",
        "                image_features_list.append(features.mean(dim=0))\n",
        "            image_features = torch.stack(image_features_list)\n",
        "\n",
        "            outputs = model(image_features, questions, attention_masks, num_questions_per_image)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(answers.cpu().numpy())\n",
        "\n",
        "            for i in range(len(predicted)):\n",
        "                question_type = question_types[i].item()\n",
        "                question_type_correct[question_type] += (predicted[i] == answers[i]).item()\n",
        "                question_type_total[question_type] += 1\n",
        "                loss = criterion(outputs[i].unsqueeze(0), answers[i].unsqueeze(0)).item()\n",
        "                question_type_loss[question_type] += loss\n",
        "\n",
        "                total_correct += (predicted[i] == answers[i]).item()\n",
        "                total_loss += loss\n",
        "                total_samples += 1\n",
        "\n",
        "    # Calculate overall accuracy and loss\n",
        "    overall_accuracy = total_correct / total_samples\n",
        "    overall_loss = total_loss / total_samples\n",
        "    print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "    print(f\"Overall Loss: {overall_loss:.4f}\")\n",
        "\n",
        "    # Calculate accuracy and loss for each question type\n",
        "    for question_type, correct in question_type_correct.items():\n",
        "        total = question_type_total[question_type]\n",
        "        accuracy = correct / total\n",
        "        avg_loss = question_type_loss[question_type] / total\n",
        "        question_type_name = [key for key, value in question_type_mapping.items() if value == question_type][0]\n",
        "        print(f\"Accuracy for {question_type_name}: {accuracy * 100:.2f}%\")\n",
        "        print(f\"Average loss for {question_type_name}: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Z8wFdt8Aprbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run validation\n",
        "validate_model(vqa_model, test_dataloader, criterion)"
      ],
      "metadata": {
        "id": "O5buc1Hmpstk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}